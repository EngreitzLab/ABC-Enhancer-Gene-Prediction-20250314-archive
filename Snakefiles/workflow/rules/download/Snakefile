import yaml
import os


# make directories 
if not os.path.exists(config['output_data_dir']+"/input_files/"):
	os.makedirs(config['output_data_dir']+"/input_files/")

OUTPUT_FILES=[]

### List output files from rules from downloading metadata
OUTPUT_FILES.extend(expand("{config}/input_files/bam_{genome_build}_{assay}.tsv", config=config['output_data_dir'], assay=config['params_download']['assays'], genome_build=config['params_download']['genome']))
OUTPUT_FILES.extend(expand("{config}/input_files/unique_{endtype}_h3k27ac_dhs_files.tsv", config=config['output_data_dir'], endtype=config['params_download']['pairedness']))

### Overall rule that runs rule download_metadata and rule filter_metadata
rule target:
	input: OUTPUT_FILES

### This rule downloads the metadata file off ENCODE for all samples that contain both DHS/ATAC bam files and H3K27ac bam files
### Outputs metadata files in both hg19 and GRCh38
rule download_metadata:
        input:
                exe = workflow.workdir_init+config['workflow_scripts']+"download_log.sh",
		outdir = config['output_data_dir']+"/input_files/"

        message: "Executing log bash files to download files"
        
	params:	genome_build = config['params_download']['genome']

	output:	files = expand(["{config}/input_files/bam_{genome_build}_{assay}.tsv", "{config}/input_files/fastq_{genome_build}_{assay}.tsv"], config=config['output_data_dir'], assay=config['params_download']['assays'], genome_build=config['params_download']['genome'])		
	
	shell: """
		bash {input.exe}
		mv *hg19* {input.outdir}
		mv *38* {input.outdir}
		""" 

FILES = expand(["{config}/input_files/bam_{genome_build}_{assay}.tsv", "{config}/input_files/fastq_{genome_build}_{assay}.tsv"], config = config['output_data_dir'], assay=config['params_download']['assays'], genome_build=config['params_download']['genome'])
print(FILES)
# This rule filters the above metadata file and filters it on specific criterion, grabbing only files that are required for the ABC pipeline and for duplicate filtering in the preprocessing step of this pipeline
### Output input_data_lookup.json and other files that are necessary for the Snakefile in rule preprocessing as well as in the ABC Snakefile workflow. 
rule filter_metadata:
        input:
                exe = workflow.workdir_init+config['workflow_scripts']+"grabDownload.py",
		data_dir = config['output_data_dir'],
		data = expand(["{config}/input_files/bam_{genome_build}_{assay}.tsv", "{config}/input_files/fastq_{genome_build}_{assay}.tsv"], config=config['output_data_dir'],  assay=config['params_download']['assays'], genome_build=config['params_download']['genome']),
		expt_file = expand("{config}rel_experiments_20201224.tsv", config=config["working_dir"])

	params: pool = config['pooling_param'],
                threads = config['threads']

	output: expand("{config}/input_files/unique_{endtype}_h3k27ac_dhs_files.tsv", config=config['output_data_dir'], endtype=config['params_download']['pairedness'])

        shell: """
		python {input.exe} --dhs {input.data[0]} --h3k27ac {input.data[2]} --atac {input.data[4]} --dhs_fastq {input.data[1]} --h3k27ac_fastq {input.data[3]} --atac_fastq {input.data[5]} --genome_assembly hg19 --download_files {params.pool} --threads {params.threads} --outdir {input.data_dir}/input_files/ --data_outdir {input.data_dir}/data/ --expt_file {input.expt_file}
		cat {input.data_dir}/input_files/Experiments_ToCombine.txt.tmp | sed 's/[][]//g' | sed s/"'"//g | sed "s/,/   /g" > {input.data_dir}/input_files/Experiments_ToCombine.txt
		"""

